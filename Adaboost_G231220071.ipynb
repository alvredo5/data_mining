{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjMf4a8bouOH8KDT0hhDck",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvredo5/data_mining/blob/main/Adaboost_G231220071.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sVzfOPCNLm9t"
      },
      "outputs": [],
      "source": [
        "#Load libraries\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn import datasets\n",
        "#import train_test_split function\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-learn matrics module for accurary calculation\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load data\n",
        "iris = datasets.load_iris()\n",
        "X = iris\n",
        "y = iris.target"
      ],
      "metadata": {
        "id": "b7yhf6LSMgsK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "source": [
        "#load data\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  # Use iris.data for features\n",
        "y = iris.target"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1hnVZLaSZtvK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split dataset into training set and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
        "# 70% training and 30% test"
      ],
      "metadata": {
        "id": "jLXw60USaOA8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create adaboost classifier object\n",
        "abc = AdaBoostClassifier(n_estimators=50,learning_rate=1)\n",
        "# Train Adaboost Classifier\n",
        "model = abc.fit(X_train, y_train)\n",
        "# predict the  response for dataset\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "fYNCzgpCO8VX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Accurary, how often is the classifier correct?\n",
        "print(\"Accurary:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f7KGuX3Qkc4",
        "outputId": "29c98d63-3144-4400-9dbb-6d1dc4cb82ac"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accurary: 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/eriklindernoren/ML-From-Scratch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWFHl3COQ-Ua",
        "outputId": "199f3aeb-bc06-4d3e-94b6-b8471149080e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ML-From-Scratch' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/ML-Scratch')"
      ],
      "metadata": {
        "id": "URHTezP-Rc0L"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = datasets.load_digits()"
      ],
      "metadata": {
        "id": "SGZQaGi_Ro_M"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Error \"No module named 'mlfromscratch'\" menunjukkan bahwa modul\n",
        "# mlfromscratch tidak tersedia di lingkungan Python Anda. Modul ini\n",
        "# bukan bagian dari pustaka standar Python dan perlu diinstal secara\n",
        "# terpisah. Jika modul ini tidak tersedia, kita bisa membuat fungsi\n",
        "# bantu (helper functions) yang diperlukan dalam skrip yang sama.\n",
        "\n",
        "from __future__ import division, print_function\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "\n",
        "# Fungsi bantu untuk membagi data menjadi set pelatihan dan pengujian\n",
        "def train_test_split(X, y, test_size=0.2, random_state=None):\n",
        "    if random_state:  # Jika diberikan, gunakan seed untuk hasil yang konsisten\n",
        "        np.random.seed(random_state)\n",
        "    indices = np.arange(X.shape[0])  # Buat array indeks data\n",
        "    np.random.shuffle(indices)  # Acak indeks\n",
        "    test_size = int(len(indices) * test_size)  # Tentukan ukuran set pengujian\n",
        "    test_indices = indices[:test_size]  # Indeks untuk set pengujian\n",
        "    train_indices = indices[test_size:]  # Indeks untuk set pelatihan\n",
        "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
        "\n",
        "# Fungsi bantu untuk menghitung akurasi prediksi\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    accuracy = np.sum(y_true == y_pred) / len(y_true)  # Hitung akurasi\n",
        "    return accuracy\n",
        "\n",
        "# Kelas untuk plotting boundary keputusan\n",
        "class Plot:\n",
        "    @staticmethod\n",
        "    def plot_decision_boundary(clf, X, y):\n",
        "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
        "                             np.arange(y_min, y_max, 0.01))\n",
        "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "        plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')\n",
        "        plt.show()\n",
        "\n",
        "# Decision stump sebagai pengklasifikasi lemah untuk Adaboost\n",
        "class DecisionStump():\n",
        "    def __init__(self):\n",
        "        self.polarity = 1  # Polaritas untuk menentukan kelas\n",
        "        self.feature_index = None  # Indeks fitur yang digunakan untuk klasifikasi\n",
        "        self.threshold = None  # Nilai threshold untuk fitur\n",
        "        self.alpha = None  # Nilai alpha yang menunjukkan kekuatan pengklasifikasi\n",
        "\n",
        "# Adaboost\n",
        "class Adaboost():\n",
        "    \"\"\"Boosting method yang menggunakan sejumlah pengklasifikasi lemah\n",
        "    dalam ensemble untuk membuat pengklasifikasi kuat. Implementasi ini\n",
        "    menggunakan decision stumps, yang merupakan Decision Tree satu level.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    n_clf: int\n",
        "        Jumlah pengklasifikasi lemah yang akan digunakan.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clf=5):\n",
        "        self.n_clf = n_clf  # Set jumlah pengklasifikasi lemah\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = np.shape(X)  # Dapatkan jumlah sampel dan fitur\n",
        "        w = np.full(n_samples, (1 / n_samples))  # Inisialisasi bobot sampel\n",
        "        self.clfs = []  # List untuk menyimpan pengklasifikasi\n",
        "\n",
        "        for _ in range(self.n_clf):  # Untuk setiap pengklasifikasi lemah\n",
        "            clf = DecisionStump()  # Inisialisasi decision stump\n",
        "            min_error = float('inf')  # Inisialisasi error minimum\n",
        "\n",
        "            for feature_i in range(n_features):  # Untuk setiap fitur\n",
        "                feature_values = np.expand_dims(X[:, feature_i], axis=1)  # Ekspansi dimensi fitur\n",
        "                unique_values = np.unique(feature_values)  # Dapatkan nilai unik fitur\n",
        "\n",
        "                for threshold in unique_values:  # Untuk setiap nilai threshold\n",
        "                    p = 1\n",
        "                    prediction = np.ones(n_samples)  # Prediksi awal semua 1\n",
        "                    prediction[X[:, feature_i] < threshold] = -1  # Ubah prediksi jika kurang dari threshold\n",
        "\n",
        "                    error = sum(w[y != prediction])  # Hitung error\n",
        "\n",
        "                    if error > 0.5:  # Jika error lebih dari 0.5, ubah polaritas\n",
        "                        error = 1 - error\n",
        "                        p = -1\n",
        "\n",
        "                    if error < min_error:  # Simpan pengaturan terbaik\n",
        "                        clf.polarity = p\n",
        "                        clf.threshold = threshold\n",
        "                        clf.feature_index = feature_i\n",
        "                        min_error = error\n",
        "\n",
        "            EPS = 1e-10  # Nilai epsilon kecil untuk mencegah pembagian oleh nol\n",
        "            clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))  # Hitung alpha\n",
        "            predictions = np.ones(n_samples)  # Prediksi awal semua 1\n",
        "            negative_idx = (clf.polarity * X[:, clf.feature_index] < clf.polarity * clf.threshold)\n",
        "            predictions[negative_idx] = -1  # Ubah prediksi jika kurang dari threshold\n",
        "\n",
        "            w *= np.exp(-clf.alpha * y * predictions)  # Perbarui bobot\n",
        "            w /= np.sum(w)  # Normalisasi bobot\n",
        "            self.clfs.append(clf)  # Tambahkan pengklasifikasi ke list\n",
        "\n",
        "    def predict(self, X):\n",
        "        clf_preds = [clf.alpha * (clf.polarity * X[:, clf.feature_index] < clf.polarity * clf.threshold).astype(int)\n",
        "                     for clf in self.clfs]  # Kumpulkan prediksi dari semua pengklasifikasi\n",
        "        y_pred = np.sum(clf_preds, axis=0)  # Jumlahkan prediksi\n",
        "        y_pred = np.sign(y_pred)  # Ambil tanda dari hasil penjumlahan\n",
        "        return y_pred\n",
        "\n",
        "# Contoh penggunaan\n",
        "if __name__ == \"__main__\":\n",
        "    data = datasets.load_breast_cancer()  # Load dataset kanker payudara\n",
        "    X = data.data  # Fitur\n",
        "    y = data.target  # Label\n",
        "    y[y == 0] = -1  # Ubah label 0 menjadi -1\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)  # Bagi data menjadi set pelatihan dan pengujian\n",
        "\n",
        "    clf = Adaboost(n_clf=5)  # Inisialisasi Adaboost dengan 5 pengklasifikasi lemah\n",
        "    clf.fit(X_train, y_train)  # Latih model\n",
        "    y_pred = clf.predict(X_test)  # Prediksi set pengujian\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)  # Hitung akurasi\n",
        "    print(\"Accuracy:\", accuracy)  # Cetak akurasi\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPf2ojDkS4Vj",
        "outputId": "facf37b1-cb44-4b36-a90e-91fe84a09b7c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.39823008849557523\n"
          ]
        }
      ]
    }
  ]
}